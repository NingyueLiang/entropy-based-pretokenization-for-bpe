{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical model for pre-tokenization\n",
        "\n",
        "## Pointwise mutual information and branching entropy to identify coherent character spans\n",
        "\n",
        "This notebook implements statistical methods to identify natural word boundaries in Chinese text:\n",
        "1. Pointwise Mutual Information (PMI) measures the association strength between adjacent characters\n",
        "2. Branching Entropy quantifies the uncertainty of the next character given the current context\n",
        "\n",
        "These metrics help identify where characters form coherent units (words) by:\n",
        "- High PMI indicates strong character associations likely to form words\n",
        "- Low branching entropy suggests predictable character sequences typical of words\n",
        "\n",
        "The implementation uses these statistical measures to guide pre-tokenization before applying tokenization methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Qr0ShxHV4uV"
      },
      "outputs": [],
      "source": [
        "def extract_first_1000_lines(input_file_path, output_file_path):\n",
        "    \"\"\"\n",
        "    Extracts the first 1000 lines from a UTF-8 encoded file.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): The path to the input UTF-8 file.\n",
        "        output_file_path (str): The path where the first 1000 lines will be saved.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "\n",
        "        line_count = 0\n",
        "        for line in infile:\n",
        "            outfile.write(line)\n",
        "            line_count += 1\n",
        "            if line_count >= 1000:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm-4VH3wiElb"
      },
      "outputs": [],
      "source": [
        "extract_first_1000_lines(\"pku_training.utf8\", \"pku_small.utf8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU85qQvkiGa3"
      },
      "outputs": [],
      "source": [
        "def remove_spaces(input_file_path, output_file_path):\n",
        "    \"\"\"\n",
        "    Removes all space characters (' ') from each line of the input file and\n",
        "    writes the modified text to the output file.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): Path to the input file (UTF-8 encoding).\n",
        "        output_file_path (str): Path to save the resulting file (UTF-8 encoding).\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            no_spaces_line = line.replace(' ', '')\n",
        "            outfile.write(no_spaces_line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbB3RIb1iIWB"
      },
      "outputs": [],
      "source": [
        "remove_spaces(\"pku_small.utf8\", \"no_spaces_pku.utf8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Statistical Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Greedy word segmentation by total_score = minPMI + min(left_entropy, right_entropy).\n",
        "1. No hard thresholding. All multi-character candidates are kept; their\n",
        "   scores are used directly during greedy segmentation.\n",
        "2. Single pass statistics collection. We walk through the corpus once\n",
        "   (up to max_len characters per position) and gather:\n",
        "   * n-gram frequencies (for PMI)\n",
        "   * left / right context character counts (for entropy)\n",
        "   This avoids repeated re.findall calls.\n",
        "3. Clearer data flow. Helper functions do exactly one job, making dependencies explicit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/liangningyue/opt/miniconda3/envs/cse217a/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "import math\n",
        "import sys\n",
        "import re\n",
        "from collections import Counter, defaultdict, Set\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Sequence, Tuple\n",
        "import sentencepiece as spm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Below is the script for the statistical model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kp6omCmiLDg"
      },
      "outputs": [],
      "source": [
        "# Global containers required for later analysis / plotting\n",
        "pmi_score_dict: Dict[str, float] = {}\n",
        "# word -> (left_entropy, right_entropy)\n",
        "entropy_dict: Dict[str, Tuple[float, float]] = {}\n",
        "\n",
        "# Corpus statistics helpers\n",
        "def collect_corpus_stats(text: str, max_len: int) -> tuple[Counter, dict[str, Counter], dict[str, Counter]]:\n",
        "    \"\"\"Walk through text once and gather statistics needed later.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        Raw corpus with stop characters already removed.\n",
        "    max_len : int\n",
        "        Maximum n-gram length considered.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    freq : Counter\n",
        "        word -> frequency for all n-grams 1 .. max_len.\n",
        "    left_ctx : dict[str, Counter]\n",
        "        word -> Counter(left_char) (only positions with both a left and a\n",
        "        right neighbour are counted, matching the original regex behaviour).\n",
        "    right_ctx : dict[str, Counter]\n",
        "        word -> Counter(right_char) (see remark above).\n",
        "    \"\"\"\n",
        "    freq: Counter = Counter()\n",
        "    left_ctx: dict[str, Counter] = defaultdict(Counter)\n",
        "    right_ctx: dict[str, Counter] = defaultdict(Counter)\n",
        "\n",
        "    n = len(text)\n",
        "    for i in range(n):\n",
        "        # Limit the slice to stay inside text\n",
        "        slice_end = min(i + max_len, n)\n",
        "        for j in range(i + 1, slice_end + 1):\n",
        "            w = text[i:j]\n",
        "            freq[w] += 1\n",
        "\n",
        "            # Context only if there is both a left & right neighbour\n",
        "            if i > 0 and j < n:\n",
        "                left_ctx[w][text[i - 1]] += 1\n",
        "                right_ctx[w][text[j]] += 1\n",
        "\n",
        "    return freq, left_ctx, right_ctx\n",
        "\n",
        "# Score computation (minPMI + min LR-entropy)\n",
        "\n",
        "def compute_min_pmi(freq: Counter, total_count: int) -> None:\n",
        "    \"\"\"Populate pmi_score_dict with the min-PMI of each multi-char word.\"\"\"\n",
        "    pmi_score_dict.clear()\n",
        "\n",
        "    for word, f_xy in freq.items():\n",
        "        if len(word) == 1:\n",
        "            continue  # single characters are skipped (as in the original code)\n",
        "\n",
        "        # Gather PMI for every bi-partition of word\n",
        "        pmi_values: list[float] = []\n",
        "        for i in range(1, len(word)):\n",
        "            left, right = word[:i], word[i:]\n",
        "            f_x = freq.get(left, 0)\n",
        "            f_y = freq.get(right, 0)\n",
        "            if not f_x or not f_y:\n",
        "                pmi_values.append(float(\"-inf\"))\n",
        "            else:\n",
        "                ratio = (f_xy * total_count) / (f_x * f_y)\n",
        "                pmi_values.append(math.log2(ratio))\n",
        "\n",
        "        pmi_score_dict[word] = min(pmi_values) if pmi_values else float(\"-inf\")\n",
        "\n",
        "\n",
        "def entropy_from_counter(counter: Counter) -> float:\n",
        "    \"\"\"Shannon entropy H(X) with log-base 2.\"\"\"\n",
        "    total = sum(counter.values())\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    return -sum((c / total) * math.log2(c / total) for c in counter.values())\n",
        "\n",
        "\n",
        "def compute_total_scores(\n",
        "    candidate_words: Sequence[str],\n",
        "    left_ctx: dict[str, Counter],\n",
        "    right_ctx: dict[str, Counter],\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"Return word -> total_score and populate entropy_dict for plotting.\"\"\"\n",
        "    entropy_dict.clear()\n",
        "    score_dict: dict[str, float] = {}\n",
        "\n",
        "    for w in candidate_words:\n",
        "        le = entropy_from_counter(left_ctx.get(w, Counter()))\n",
        "        re = entropy_from_counter(right_ctx.get(w, Counter()))\n",
        "        min_lr = min(le, re)\n",
        "        entropy_dict[w] = (le, re)\n",
        "\n",
        "        pmi = pmi_score_dict.get(w, float(\"-inf\"))\n",
        "        score_dict[w] = pmi + min_lr  # weighting factor 1.0 as in original script\n",
        "\n",
        "    return score_dict\n",
        "\n",
        "# Greedy segmentation with conflict resolution\n",
        "def build_conflict_free_seg(text: str, word_score: dict[str, float]) -> list[tuple[int, int, str]]:\n",
        "    \"\"\"Greedy selection identical to the original logic.\n",
        "\n",
        "    The tuple returned for each chosen occurrence is (start, end, word).\n",
        "    \"\"\"\n",
        "    matches: list[tuple[int, int, str, float]] = []\n",
        "\n",
        "    for w, score in word_score.items():\n",
        "        if score == float(\"-inf\"):\n",
        "            continue  # useless candidate\n",
        "        start = 0\n",
        "        while True:\n",
        "            idx = text.find(w, start)\n",
        "            if idx == -1:\n",
        "                break\n",
        "            matches.append((idx, idx + len(w), w, score))\n",
        "            start = idx + 1  # allow overlaps in search as in original code\n",
        "\n",
        "    # Sort: primary by start idx, then by descending score, then by descending length\n",
        "    matches.sort(key=lambda t: (t[0], -t[3], -(t[1] - t[0])))\n",
        "\n",
        "    chosen: list[tuple[int, int, str, float]] = []\n",
        "    for st, ed, wd, sc in matches:\n",
        "        for i, (cst, ced, _, csc) in enumerate(chosen):\n",
        "            if not (ed <= cst or st >= ced):  # overlap\n",
        "                if sc > csc:\n",
        "                    chosen[i] = (st, ed, wd, sc)\n",
        "                break\n",
        "        else:\n",
        "            chosen.append((st, ed, wd, sc))\n",
        "\n",
        "    chosen.sort(key=lambda t: t[0])\n",
        "    return [(st, ed, wd) for st, ed, wd, _ in chosen]\n",
        "\n",
        "def _plot_hist(data: List[float], title: str, xlabel: str, bins: int = 50, xlim: Tuple[float, float] | None = None) -> None:\n",
        "    plt.figure()\n",
        "    counts, bin_edges, patches = plt.hist(data, bins=bins, range=xlim, edgecolor=\"black\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "    for cnt, rect in zip(counts, patches):\n",
        "        if cnt > 0:\n",
        "            x_pos = rect.get_x() + rect.get_width() / 2\n",
        "            plt.text(x_pos, cnt, f\"{int(cnt)}\", ha=\"center\", va=\"bottom\", rotation=90, fontsize=8)\n",
        "\n",
        "    if xlim:\n",
        "        plt.xlim(xlim)\n",
        "\n",
        "\n",
        "def plot_distributions() -> None:\n",
        "    \"\"\"Show histograms for PMI and entropy statistics.\"\"\"\n",
        "    pmi_vals = [v for v in pmi_score_dict.values() if v != float(\"-inf\")]\n",
        "    left_vals, right_vals, min_vals = [], [], []\n",
        "    for le, re in entropy_dict.values():\n",
        "        left_vals.append(le)\n",
        "        right_vals.append(re)\n",
        "        min_vals.append(min(le, re))\n",
        "\n",
        "    # PMI\n",
        "    _plot_hist(pmi_vals, \"PMI distribution (full range)\", \"PMI\")\n",
        "    _plot_hist(pmi_vals, \"PMI distribution (zoomed)\", \"PMI\", xlim=(-1, 5))\n",
        "\n",
        "    # Entropy\n",
        "    _plot_hist(left_vals, \"Left entropy (full range)\", \"Left entropy\")\n",
        "    _plot_hist(left_vals, \"Left entropy (zoomed)\", \"Left entropy\", xlim=(0, 2))\n",
        "    _plot_hist(right_vals, \"Right entropy (full range)\", \"Right entropy\")\n",
        "    _plot_hist(right_vals, \"Right entropy (zoomed)\", \"Right entropy\", xlim=(0, 2))\n",
        "    _plot_hist(min_vals, \"Min(L,R) entropy (full range)\", \"Min entropy\")\n",
        "    _plot_hist(min_vals, \"Min(L,R) entropy (zoomed)\", \"Min entropy\", xlim=(0, 1))\n",
        "\n",
        "def main() -> None:\n",
        "    # Load corpus & basic cleanup\n",
        "    stop_chars: Sequence[str] = [\n",
        "        \"【\", \"】\", \")\", \"(\", \"、\", \"，\", \"“\", \"”\", \"。\", \"\\n\", \"《\", \"》\", \" \", \"-\", \"！\", \"？\",\n",
        "        \".\", \"'\", \"[\", \"]\", \"：\", \"/\", '\"', \"\\u3000\", \"’\", \"．\", \",\", \"…\", \"?\", \"（\", \"）\",\n",
        "    ]\n",
        "    raw_path = Path(\"no_spaces_pku.utf8\")\n",
        "    text = raw_path.read_text(encoding=\"utf-8\")\n",
        "    for ch in stop_chars:\n",
        "        text = text.replace(ch, \"\")\n",
        "\n",
        "    # Collect statistics\n",
        "    max_ngram = 6\n",
        "    freq, left_ctx, right_ctx = collect_corpus_stats(text, max_ngram)\n",
        "    print(f\"Total n‑gram types: {len(freq):,}\")\n",
        "\n",
        "\n",
        "    # Scores: minPMI + minLR‑entropy (no thresholding)\n",
        "    compute_min_pmi(freq, sum(freq.values()))\n",
        "    candidate_words = [w for w in freq if len(w) > 1]\n",
        "    score_dict = compute_total_scores(candidate_words, left_ctx, right_ctx)\n",
        "\n",
        "    # Greedy segmentation\n",
        "    segments = build_conflict_free_seg(text, score_dict)\n",
        "\n",
        "    # Stitch back to a tokenised string (single chars for uncovered spans)\n",
        "    segments.sort(key=lambda t: t[0])\n",
        "    tokens: list[str] = []\n",
        "    cursor = 0\n",
        "    for st, ed, wd in segments:\n",
        "        if st > cursor:\n",
        "            tokens.extend(text[cursor:st])\n",
        "        tokens.append(wd)\n",
        "        cursor = ed\n",
        "    if cursor < len(text):\n",
        "        tokens.extend(text[cursor:])\n",
        "\n",
        "    segmented = \" \".join(tokens)\n",
        "    print(\"Segmented sample:\", segmented[:200], \"...\")\n",
        "\n",
        "    Path(\"pred_weight_1.utf8\").write_text(segmented + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Functions\n",
        "Normalize a gold segmentation file that mistakenly uses two (or more)\n",
        "spaces between tokens so that it contains exactly one ASCII space\n",
        "between tokens, no leading/trailing spaces per line, and preserves the\n",
        "original number of lines. Both input and output are assumed to be UTF‑8.\n",
        "\n",
        "Edit the two path variables (*INPUT_PATH* and *OUTPUT_PATH*) as needed\n",
        "and run the script directly; no command‑line arguments are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written cleaned file to: data/pku_small_sentence.txt (lines: 2255)\n"
          ]
        }
      ],
      "source": [
        "INPUT_PATH = Path(\"data/pku_small_sentence.txt\")   # source file (two spaces)\n",
        "OUTPUT_PATH = Path(\"data/pku_small_sentence.txt\")  # destination file (one space)\n",
        "\n",
        "_WS_PATTERN = re.compile(r\"\\s+\")  # collapse every run of whitespace\n",
        "\n",
        "\n",
        "def normalize_line(line: str) -> str:\n",
        "    \"\"\"Convert all runs of whitespace into a single ASCII space.\"\"\"\n",
        "    # Replace full‑width spaces with ASCII for safety\n",
        "    line = line.replace(\"\\u3000\", \" \")\n",
        "    tokens = _WS_PATTERN.split(line.strip())\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    raw_lines = INPUT_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
        "    with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as fw:\n",
        "        for raw in raw_lines:\n",
        "            cleaned = normalize_line(raw)\n",
        "            fw.write(cleaned + \"\\n\")\n",
        "    print(f\"Written cleaned file to: {OUTPUT_PATH} (lines: {len(raw_lines)})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation \n",
        "## F1 Score \n",
        "We evaluate the F1 score of our entropy & statistical models with SIGHAN style (exact match matters) and boundary F-1 Score.\n",
        "### F1 Score Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "def lcs_length(a: List[str], b: List[str]) -> int:\n",
        "    \"\"\"Length of the longest common subsequence between two token lists.\"\"\"\n",
        "    return sum(triple.size for triple in SequenceMatcher(None, a, b).get_matching_blocks())\n",
        "\n",
        "def compute_sighan_f1(\n",
        "    gold_path,\n",
        "    pred_path\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute token-level Precision, Recall, and F1 for Chinese word segmentation\n",
        "    in SIGHAN style (one sentence per line, tokens separated by ASCII spaces).\n",
        "\n",
        "    Args:\n",
        "        gold_path: Path to the gold standard file.\n",
        "        pred_path: Path to the predictions file.\n",
        "\n",
        "    Returns:\n",
        "        precision, recall, f1\n",
        "    \"\"\"\n",
        "    gold_lines = Path(gold_path).read_text(encoding=\"utf-8\").splitlines()\n",
        "    pred_lines = Path(pred_path).read_text(encoding=\"utf-8\").splitlines()\n",
        "\n",
        "    if len(gold_lines) != len(pred_lines):\n",
        "        raise ValueError(f\"Line count mismatch: gold={len(gold_lines)} vs pred={len(pred_lines)}\")\n",
        "\n",
        "    total_gold = total_pred = total_corr = 0\n",
        "    for g_line, p_line in zip(gold_lines, pred_lines):\n",
        "        g_tokens = g_line.strip().split()\n",
        "        p_tokens = p_line.strip().split()\n",
        "        total_gold += len(g_tokens)\n",
        "        total_pred += len(p_tokens)\n",
        "        total_corr += lcs_length(g_tokens, p_tokens)\n",
        "\n",
        "    if total_pred == 0 or total_gold == 0:\n",
        "        raise ValueError(\"Gold or prediction is empty — cannot compute metrics.\")\n",
        "\n",
        "    precision = total_corr / total_pred\n",
        "    recall = total_corr / total_gold\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
        "\n",
        "    return precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1822 sentences. Saved to file 'data/pku_small_sentence.txt'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1822"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def process_sentences(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process a raw text file by splitting it into sentences at punctuation marks\n",
        "    and write each sentence to a new line in the output file.\n",
        "    \n",
        "    Args:\n",
        "        input_file (str): Path to the input raw text file\n",
        "        output_file (str): Path to the output file where sentences will be written\n",
        "    \n",
        "    Returns:\n",
        "        int: Number of sentences processed\n",
        "    \"\"\"\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Define a regex pattern that splits on sentence-ending punctuation\n",
        "    pattern = r'(?<=[。！？])'\n",
        "\n",
        "    # Split the text on the defined punctuation\n",
        "    sentences = re.split(pattern, text)\n",
        "\n",
        "    # Clean the sentences by stripping whitespace and filter out any empty strings\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Write the sentences into the output file, each sentence on a new line\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + \"\\n\")\n",
        "\n",
        "    print(f\"Processed {len(sentences)} sentences. Saved to file '{output_file}'.\")\n",
        "    return len(sentences)\n",
        "\n",
        "\n",
        "process_sentences(\"data/pku_small.utf8\", \"data/pku_small_sentence.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BPE\n",
        "\n",
        "We trained the BPE model based on our pre-tokenization outputs and then test for F1-Score on our test dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChineseBPEProcessor:\n",
        "    \"\"\"\n",
        "    Modular class to train a character-level BPE tokenizer on a Chinese corpus,\n",
        "    apply it to pre-tokenize text, and evaluate segmentation via boundary F1 & Sighan F1.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_prefix: str,\n",
        "        vocab_size: int,\n",
        "        input_path: str,\n",
        "        tokenized_path: str,\n",
        "        gold_path: str\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_prefix:    Prefix for the SentencePiece model files (will write\n",
        "                             `{model_prefix}.model` & `{model_prefix}.vocab`).\n",
        "            vocab_size:      Desired BPE vocabulary size.\n",
        "            input_path:      Raw corpus to train on, one sentence per line.\n",
        "            tokenized_path:  Output path for the tokenized text.\n",
        "            gold_path:       Path to gold segmented file (spaces between tokens).\n",
        "        \"\"\"\n",
        "        self.model_prefix   = model_prefix\n",
        "        self.vocab_size     = vocab_size\n",
        "        self.input_path     = input_path\n",
        "        self.tokenized_path = tokenized_path\n",
        "        self.gold_path      = gold_path\n",
        "        self.model_file     = f\"{model_prefix}.model\"\n",
        "        \n",
        "    def split_train_test(self):\n",
        "        \"\"\"Split input data and gold data into training and test sets.\"\"\"\n",
        "        # Split input data\n",
        "        with open(self.input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Calculate split point for 70/30 split\n",
        "        split_idx = int(len(lines) * 0.3)\n",
        "        \n",
        "        # Split into train and test sets\n",
        "        train_data = lines[split_idx:]\n",
        "        test_data = lines[:split_idx]\n",
        "        \n",
        "        train_path = f\"{self.model_prefix}_train.txt\"\n",
        "        with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.writelines(train_data)\n",
        "            \n",
        "        test_path = f\"{self.model_prefix}_test.txt\"\n",
        "        with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.writelines(test_data)\n",
        "            \n",
        "        # Split gold data\n",
        "        with open(self.gold_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            gold_lines = f.readlines()\n",
        "            \n",
        "        # Split gold data using same split point\n",
        "        gold_train_data = gold_lines[split_idx:]\n",
        "        gold_test_data = gold_lines[:split_idx]\n",
        "        \n",
        "        # Write gold train data\n",
        "        gold_train_path = f\"{self.model_prefix}_train_gold.txt\"\n",
        "        with open(gold_train_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.writelines(gold_train_data)\n",
        "            \n",
        "        # Write gold test data\n",
        "        gold_test_path = f\"{self.model_prefix}_test_gold.txt\"\n",
        "        with open(gold_test_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.writelines(gold_test_data)\n",
        "            \n",
        "        # Store paths as instance variables\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.gold_train_path = gold_train_path\n",
        "        self.gold_test_path = gold_test_path\n",
        "        \n",
        "        print(\"Split data into training and test lines\")\n",
        "\n",
        "    \n",
        "    def train(self, unk_piece: str = '[UNK]') -> None:\n",
        "        \"\"\"Train a BPE tokenizer with SentencePiece.\"\"\"\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=self.train_path,\n",
        "            model_prefix=self.model_prefix,\n",
        "            vocab_size=self.vocab_size,\n",
        "            model_type=\"bpe\",\n",
        "            character_coverage=1.0,\n",
        "            unk_piece=unk_piece,\n",
        "            pad_id=0, unk_id=1,\n",
        "            bos_id=-1, eos_id=-1,\n",
        "            user_defined_symbols=''\n",
        "        )\n",
        "        print(f\"Trained BPE model: {self.model_file}\")\n",
        "    \n",
        "    def tokenize(self) -> None:\n",
        "        \"\"\"\n",
        "        Use the trained SentencePiece model to tokenize each line of input,\n",
        "        join subword pieces with spaces, and write to `self.tokenized_path`.\n",
        "        \"\"\"\n",
        "        sp = spm.SentencePieceProcessor()\n",
        "        sp.load(self.model_file)\n",
        "        \n",
        "        os.makedirs(os.path.dirname(self.tokenized_path), exist_ok=True)\n",
        "        with open(self.test_path,  \"r\", encoding=\"utf-8\") as fin, \\\n",
        "             open(self.tokenized_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    #fout.write(\"\\n\")\n",
        "                    continue\n",
        "                piece_ids = sp.encode(line, out_type=int)\n",
        "                pieces = [sp.id_to_piece(pid) for pid in piece_ids]\n",
        "                # Strip leading underscore in first piece if present\n",
        "                # 1) Replace each leading '▁' with a space\n",
        "                processed = []\n",
        "                for p in pieces:\n",
        "                    if p.startswith('▁'):\n",
        "                        p = \" \" + p[1:]\n",
        "                    processed.append(p)\n",
        "\n",
        "                # 2) Join into one string, then collapse multiple spaces to a single space\n",
        "                text = \" \".join(processed)\n",
        "                text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "                fout.write(text + \"\\n\")\n",
        "        print(f\"Tokenized output saved to: {self.tokenized_path}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def boundaries_from_words(words: List[str]) -> set[int]:\n",
        "        \"\"\"\n",
        "        Compute boundary positions (cumulative char offsets) from a word list.\n",
        "        \"\"\"\n",
        "        bnds, cum = set(), 0\n",
        "        for w in words[:-1]:\n",
        "            cum += len(w)\n",
        "            bnds.add(cum)\n",
        "        return bnds\n",
        "    \n",
        "    @staticmethod\n",
        "    def boundary_prf(gold_seg: str, pred_seg: str) -> Tuple[float, float, float]:\n",
        "        \"\"\"\n",
        "        Compute boundary-level Precision, Recall, F1 (percent 0 - 100).\n",
        "        \"\"\"\n",
        "        gold_words = re.split(r\"\\s+\", gold_seg.strip())\n",
        "        pred_words = re.split(r\"\\s+\", pred_seg.strip())\n",
        "        gold_b = ChineseBPEProcessor.boundaries_from_words(gold_words)\n",
        "        pred_b = ChineseBPEProcessor.boundaries_from_words(pred_words)\n",
        "        if not gold_b and not pred_b:\n",
        "            return 100.0, 100.0, 100.0\n",
        "        if not gold_b or not pred_b:\n",
        "            return 0.0, 0.0, 0.0\n",
        "        correct = len(gold_b & pred_b)\n",
        "        p = correct / len(pred_b) * 100\n",
        "        r = correct / len(gold_b) * 100\n",
        "        f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
        "        return p, r, f1\n",
        "    \n",
        "    def evaluate(self) -> Tuple[float, float, float]:\n",
        "        \"\"\"\n",
        "        Read gold and tokenized files, compute per-line boundary PRF,\n",
        "        and return average Precision, Recall, F1.\n",
        "        \"\"\"\n",
        "        with open(self.gold_test_path,      \"r\", encoding=\"utf-8\") as f_gold, \\\n",
        "             open(self.tokenized_path, \"r\", encoding=\"utf-8\") as f_pred:\n",
        "            gold_lines = f_gold.readlines()\n",
        "            pred_lines = f_pred.readlines()\n",
        "        \n",
        "        # align lengths\n",
        "        assert len(gold_lines) == len(pred_lines), \\\n",
        "            \"Gold & pred line counts differ\"\n",
        "        \n",
        "        ps, rs, fs = [], [], []\n",
        "        for g, p in zip(gold_lines, pred_lines):\n",
        "            g, p = g.strip(), p.strip()\n",
        "            if not g or not p:\n",
        "                continue\n",
        "            pr, rc, f1 = self.boundary_prf(g, p)\n",
        "            ps.append(pr)\n",
        "            rs.append(rc)\n",
        "            fs.append(f1)\n",
        "        \n",
        "        avg_p  = sum(ps)/len(ps) if ps else 0.0\n",
        "        avg_r  = sum(rs)/len(rs) if rs else 0.0\n",
        "        avg_f1 = sum(fs)/len(fs) if fs else 0.0\n",
        "        \n",
        "        print(f\"Avg Precision: {avg_p:.2f}\")\n",
        "        print(f\"Avg Recall:    {avg_r:.2f}\")\n",
        "        print(f\"Avg F1:        {avg_f1:.2f}\")\n",
        "        print(f\"{avg_p:.2f},{avg_r:.2f},{avg_f1:.2f},{self.vocab_size}\")\n",
        "        return avg_p, avg_r, avg_f1\n",
        "    \n",
        "    \n",
        "def eval_sighan_f1(self) -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate token level Precision, Recall, and F1 using SIGHAN style metrics.\n",
        "    \"\"\"\n",
        "    precision, recall, f1 = compute_sighan_f1(self.gold_test_path, self.tokenized_path)\n",
        "    print(f\"SIGHAN Precision: {precision*100:.2f}\")\n",
        "    print(f\"SIGHAN Recall:    {recall*100:.2f}\")\n",
        "    print(f\"SIGHAN F1:        {f1*100:.2f}\")\n",
        "    print(f\"{precision*100:.2f},{recall*100:.2f},{f1*100:.2f},{self.vocab_size}\")\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "ChineseBPEProcessor.eval_sighan_f1 = eval_sighan_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vocab Size = 12000\n",
        "#### Baseline BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/ori_bpe_12k.model\n",
            "Tokenized output saved to: data/ori_seg_12k.txt\n",
            "SIGHAN Precision: 46.89\n",
            "SIGHAN Recall:    51.96\n",
            "SIGHAN F1:        49.30\n",
            "46.89,51.96,49.30,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.468929198361615, 0.5196135641574272, 0.4929720419524498)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/ori_bpe_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/no_spaces_sentences.txt\",\n",
        "    tokenized_path=\"data/ori_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### GPT2 Pre-tokenization + BPE \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/gpt_bpe_12k.model\n",
            "Tokenized output saved to: data/gpt_seg_12k.txt\n",
            "SIGHAN Precision: 52.07\n",
            "SIGHAN Recall:    64.69\n",
            "SIGHAN F1:        57.70\n",
            "52.07,64.69,57.70,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5207474294065453, 0.6468910069376904, 0.5770053785206177)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/gpt_bpe_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/segment_out.txt\",\n",
        "    tokenized_path=\"data/gpt_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Only PMI pre-tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/pred_0_12k.model\n",
            "Tokenized output saved to: data/pred_0_seg_12k.txt\n",
            "SIGHAN Precision: 28.69\n",
            "SIGHAN Recall:    42.92\n",
            "SIGHAN F1:        34.39\n",
            "28.69,42.92,34.39,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.28694672042311525, 0.42916423523309344, 0.3439334892179787)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/pred_0_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/pred_0_split.utf8\",\n",
        "    tokenized_path=\"data/pred_0_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  total_score = minPMI +  $\\lambda$ min(left_entropy, right_entropy)\n",
        "\n",
        "#### $\\lambda = 1$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/pred_1_12k.model\n",
            "Tokenized output saved to: data/pred_1_seg_12k.txt\n",
            "SIGHAN Precision: 41.24\n",
            "SIGHAN Recall:    55.91\n",
            "SIGHAN F1:        47.47\n",
            "41.24,55.91,47.47,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4123864179818269, 0.5591000453867601, 0.4746649051826164)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/pred_1_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/pred_1_split.utf8\",\n",
        "    tokenized_path=\"data/pred_1_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\lambda = 4$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/pred_4_12k.model\n",
            "Tokenized output saved to: data/pred_4_seg_12k.txt\n",
            "SIGHAN Precision: 54.21\n",
            "SIGHAN Recall:    64.06\n",
            "SIGHAN F1:        58.73\n",
            "54.21,64.06,58.73,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5421124828532236, 0.6406016987615898, 0.5872563005230623)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/pred_4_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/pred_4_split.utf8\",\n",
        "    tokenized_path=\"data/pred_4_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        "    \n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\lambda = 15$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/pred_15_12k.model\n",
            "Tokenized output saved to: data/pred_15_seg_12k.txt\n",
            "SIGHAN Precision: 52.83\n",
            "SIGHAN Recall:    62.17\n",
            "SIGHAN F1:        57.12\n",
            "52.83,62.17,57.12,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5283195592286501, 0.6217337742332879, 0.5712328359098086)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/pred_15_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/pred_15_split.utf8\",\n",
        "    tokenized_path=\"data/pred_15_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entropy Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split data into training and test lines\n",
            "Trained BPE model: model/pred_only_entropy_12k.model\n",
            "Tokenized output saved to: data/pred_only_entropy_seg_12k.txt\n",
            "SIGHAN Precision: 51.28\n",
            "SIGHAN Recall:    60.98\n",
            "SIGHAN F1:        55.71\n",
            "51.28,60.98,55.71,12000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5128414853590708, 0.6098035401672827, 0.55713524080327)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor = ChineseBPEProcessor(\n",
        "    model_prefix=\"model/pred_only_entropy_12k\",\n",
        "    vocab_size=12000,\n",
        "    input_path=\"data/pred_only_entropy_split.utf8\",\n",
        "    tokenized_path=\"data/pred_only_entropy_seg_12k.txt\",\n",
        "    gold_path=\"data/gold_pku_test.txt\"\n",
        ")\n",
        "processor.split_train_test()\n",
        "processor.train()\n",
        "processor.tokenize()\n",
        "processor.eval_sighan_f1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cse217a",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
